{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para converter o arquivo em um formato colunar de alta performance de leitura os dados escolhi o Parquet por ter mais uma literatura mais ampla<br>\n",
    "- E para o desenvolvimento escolhi o Jupyter, por ter mais familiaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "class PySparkTest():\n",
    "    def __init__(self, file_in=None, file_out=None, file_schema=None):\n",
    "        self.file_in = file_in\n",
    "        self.file_out = file_out\n",
    "        self.file_schema = file_schema\n",
    "        self.spark = SparkSession.builder.appName(\"PySparkTest\").getOrCreate()\n",
    "        self.df = None\n",
    "        \n",
    "    def _data_deduplication(self, file_in=None):\n",
    "        path = ''        \n",
    "        if file_in == None:\n",
    "            path = self.file_in\n",
    "            \n",
    "        df = self.spark.read.csv(path, header=True)\n",
    "        df.createOrReplaceTempView('user')\n",
    "        df_sql = self.spark.sql('''SELECT * \n",
    "                                FROM user where (id, update_date) in (SELECT id, max(update_date) \n",
    "                                                                        FROM user group by id)''')\n",
    "        self.df = df_sql\n",
    "        \n",
    "    def _convert_column(self, file_schema=None):\n",
    "        path = ''\n",
    "        if file_schema == None:\n",
    "            path = self.file_schema\n",
    "            \n",
    "        if self.df == None:\n",
    "            print('Arquivo de entrada não encontrado')\n",
    "            return\n",
    "            \n",
    "        df_json = self.spark.read.option(\"multiline\", \"true\").json(path)\n",
    "        for x in df_json.schema.names:\n",
    "            for y in df_json.rdd.collect():\n",
    "                self.df = self.df.withColumn(x, self.df[x].cast(y[x]))\n",
    "                \n",
    "    def _save_file(self, file_out=None):\n",
    "        path = ''\n",
    "        \n",
    "        if file_out == None:\n",
    "            path = self.file_out\n",
    "        \n",
    "        if self.df == None:\n",
    "            print('Não existe nada para salvar.')\n",
    "            return\n",
    "        \n",
    "        self.df.write.parquet(path, mode='overwrite')\n",
    "        \n",
    "        \n",
    "    def run_process(self):\n",
    "        self._data_deduplication()\n",
    "        self._convert_column()\n",
    "        self._save_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog = PySparkTest(file_in='data/input/users/load.csv', \n",
    "                  file_out='data/output/load.parquet', \n",
    "                  file_schema='config/types_mapping.json')\n",
    "cog.run_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
